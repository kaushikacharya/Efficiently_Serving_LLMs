{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c9c678b",
   "metadata": {},
   "source": [
    "# Lesson 4 - Quantization\n",
    "\n",
    "In this lesson, we'll discuss the concept of \"quantization\". This technique helps reduce the memory overhead of a model and enables running inference with larger LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c8653b",
   "metadata": {},
   "source": [
    "### Import required packages and load the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7fae26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaushik/miniconda3/envs/py3_efficiently_serving_llms_course/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "\n",
    "from utils import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4eb908e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaushik/miniconda3/envs/py3_efficiently_serving_llms_course/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0b6f6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_name_or_path\": \"gpt2\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.35.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0dcdada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# pad on the left so we can append new tokens on the right\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51c838d",
   "metadata": {},
   "source": [
    "### Define a Float 32 type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8be23189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix dtype post quantization to \"pretend\" to be fp32\n",
    "def get_float32_dtype(self):\n",
    "    return torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b199d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2Model.type = property(get_float32_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6493b2",
   "metadata": {},
   "source": [
    "Check memory footprint of non-quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c900ee58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510342192"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_memory_footprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54251c2e",
   "metadata": {},
   "source": [
    "### Define a quantization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59f72779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(t):\n",
    "    # obtain range of values in the tensor to map between 0 and 255\n",
    "    min_val, max_val = t.min(), t.max()\n",
    "\n",
    "    # determine the \"zero-point\", or value in the tensor to map to 0\n",
    "    scale = (max_val - min_val)/255\n",
    "    zero_point = min_val\n",
    "\n",
    "    # quantize and clamp to ensure we're in [0, 255]\n",
    "    t_quant = (t - zero_point)/scale\n",
    "    t_quant = torch.clamp(t_quant, min=0, max=255)\n",
    "\n",
    "    # keep track of scale and zero_point for reversing quantization\n",
    "    state = (scale, zero_point)\n",
    "\n",
    "    # cast to uint8 and return\n",
    "    t_quant = t_quant.type(torch.uint8)\n",
    "\n",
    "    return t_quant, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bed904e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cf83491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4738, -0.2614, -0.0978,  ...,  0.0513, -0.0584,  0.0250],\n",
      "        [ 0.0874,  0.1473,  0.2387,  ..., -0.0525, -0.0113, -0.0156],\n",
      "        [ 0.0039,  0.0695,  0.3668,  ...,  0.1143,  0.0363, -0.0318],\n",
      "        ...,\n",
      "        [-0.2592, -0.0164,  0.1991,  ...,  0.0095, -0.0516,  0.0319],\n",
      "        [ 0.1517,  0.2170,  0.1043,  ...,  0.0293, -0.0429, -0.0475],\n",
      "        [-0.4100, -0.1924, -0.2400,  ..., -0.0046,  0.0070,  0.0198]]) torch.Size([768, 2304])\n"
     ]
    }
   ],
   "source": [
    "t = model.transformer.h[0].attn.c_attn.weight.data\n",
    "print(t, t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35903ec1",
   "metadata": {},
   "source": [
    "Let's quantize and check the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a86bf612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[107, 116, 124,  ..., 130, 125, 129],\n",
      "        [132, 135, 139,  ..., 126, 128, 127],\n",
      "        [128, 131, 145,  ..., 133, 130, 127],\n",
      "        ...,\n",
      "        [116, 127, 137,  ..., 129, 126, 130],\n",
      "        [135, 138, 133,  ..., 129, 126, 126],\n",
      "        [110, 119, 117,  ..., 128, 128, 129]], dtype=torch.uint8) tensor(0, dtype=torch.uint8) tensor(255, dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "t_q, state = quantize(t)\n",
    "print(t_q, t_q.min(), t_q.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c4260d",
   "metadata": {},
   "source": [
    "### Define a dequantization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7dd1b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dequantize(t, state):\n",
    "    scale, zero_point = state\n",
    "    return t.to(torch.float32)*scale + zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c89fcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4774, -0.2783, -0.1014,  ...,  0.0313, -0.0793,  0.0092],\n",
      "        [ 0.0755,  0.1419,  0.2303,  ..., -0.0572, -0.0129, -0.0351],\n",
      "        [-0.0129,  0.0534,  0.3630,  ...,  0.0976,  0.0313, -0.0351],\n",
      "        ...,\n",
      "        [-0.2783, -0.0351,  0.1861,  ...,  0.0092, -0.0572,  0.0313],\n",
      "        [ 0.1419,  0.2082,  0.0976,  ...,  0.0092, -0.0572, -0.0572],\n",
      "        [-0.4110, -0.2120, -0.2562,  ..., -0.0129, -0.0129,  0.0092]])\n"
     ]
    }
   ],
   "source": [
    "t_rev = dequantize(t_q, state=state)\n",
    "print(t_rev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab39cabe",
   "metadata": {},
   "source": [
    "Let's check the quantization error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13547604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0035, 0.0170, 0.0036,  ..., 0.0200, 0.0209, 0.0158],\n",
      "        [0.0119, 0.0055, 0.0084,  ..., 0.0046, 0.0017, 0.0195],\n",
      "        [0.0168, 0.0161, 0.0038,  ..., 0.0167, 0.0050, 0.0032],\n",
      "        ...,\n",
      "        [0.0191, 0.0187, 0.0131,  ..., 0.0004, 0.0056, 0.0006],\n",
      "        [0.0098, 0.0088, 0.0067,  ..., 0.0202, 0.0143, 0.0097],\n",
      "        [0.0010, 0.0196, 0.0162,  ..., 0.0084, 0.0199, 0.0107]]) tensor(0.) tensor(0.0221) tensor(0.0111)\n"
     ]
    }
   ],
   "source": [
    "err = torch.abs(t_rev - t)\n",
    "print(err, err.min(), err.max(), err.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be34701a",
   "metadata": {},
   "source": [
    "Response generated by original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11e4855b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick brown fox jumped over the fence and ran to the other side of the fence',\n",
       " 'The rain in Spain falls on the first day of the month, and the',\n",
       " 'What comes up must be a good idea.\\n\\n\"I think']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_generated = generate(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    requests= [\n",
    "        (\"The quick brown fox jumped over the\", 10),\n",
    "        (\"The rain in Spain falls\", 10),\n",
    "        (\"What comes up must\",10)]\n",
    ")\n",
    "\n",
    "response_generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7481747b",
   "metadata": {},
   "source": [
    "### Let's apply the quantization technique to the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb3f3483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model):\n",
    "    states = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "        param.data, state = quantize(param)\n",
    "        states[name] = state\n",
    "    \n",
    "    return model, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75ce0478",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_model, states = quantize_model(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a7c3d2",
   "metadata": {},
   "source": [
    "Memory Footprint of Quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "774acdbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137022768"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_model.get_memory_footprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1bc63f",
   "metadata": {},
   "source": [
    "Memory Footprint of `states`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c479777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_in_bytes(t):\n",
    "    return t.numel() * t.element_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5365235b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1184"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([size_in_bytes(v[0]) + size_in_bytes(v[1]) for v in states.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e3cecc",
   "metadata": {},
   "source": [
    "Observation: `states` consume low memory compared to the memory consumed by the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7424e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dequantize_model(model, states):\n",
    "    for name, param in model.named_parameters():\n",
    "        param.data = dequantize(param.data, state=states[name])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "676e38cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dequant_model = dequantize_model(model=quant_model, states=states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f79e769a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510342192"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dequant_model.get_memory_footprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e7cf02",
   "metadata": {},
   "source": [
    "Observation: Memory footprint of dequantized model is same as the original model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be118f14",
   "metadata": {},
   "source": [
    "Response generated by the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57f7db6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick brown fox jumped over the fence.\\n\\nThe fox jumped over the fence',\n",
       " 'The rain in Spain falls on Saturday night.\\n\\nSpainSpainSpainSpain',\n",
       " 'What comes up must be what is what is what is what is what']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_generated = generate(\n",
    "    model=dequant_model,\n",
    "    tokenizer=tokenizer,\n",
    "    requests= [\n",
    "        (\"The quick brown fox jumped over the\", 10),\n",
    "        (\"The rain in Spain falls\", 10),\n",
    "        (\"What comes up must\",10)]\n",
    ")\n",
    "\n",
    "response_generated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_efficiently_serving_llms_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
